{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitpr4596/Sarcasm-Detection-/blob/main/Sarcasm_DetectionSem_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQssVJqfm2Fh"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evaq-VAuXpL6",
        "outputId": "451d6097-8584-4fa0-b36b-43d09607df73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "TF-IDF Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.82      2996\n",
            "           1       0.79      0.74      0.76      2346\n",
            "\n",
            "    accuracy                           0.80      5342\n",
            "   macro avg       0.80      0.79      0.79      5342\n",
            "weighted avg       0.80      0.80      0.80      5342\n",
            "\n",
            "Word2Vec Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Word2Vec Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78      2996\n",
            "           1       0.72      0.73      0.72      2346\n",
            "\n",
            "    accuracy                           0.76      5342\n",
            "   macro avg       0.75      0.75      0.75      5342\n",
            "weighted avg       0.76      0.76      0.76      5342\n",
            "\n",
            "GloVe Best Parameters: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "GloVe Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.82      0.79      2996\n",
            "           1       0.74      0.67      0.71      2346\n",
            "\n",
            "    accuracy                           0.75      5342\n",
            "   macro avg       0.75      0.74      0.75      5342\n",
            "weighted avg       0.75      0.75      0.75      5342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Ensure NLTK tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('/content/Sarcasm_Headlines_Dataset.xlsx')\n",
        "df['headline'] = df['headline'].astype(str)  # Ensure all headlines are strings\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Fewer values for regularization strength\n",
        "    'penalty': ['l2'],        # Use only L2 for simplicity\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "\n",
        "# ---- TF-IDF Embedding ----\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Grid search with Logistic Regression for TF-IDF features\n",
        "logreg_tfidf = LogisticRegression()\n",
        "grid_search_tfidf = GridSearchCV(logreg_tfidf, param_grid, cv=5)\n",
        "grid_search_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf = grid_search_tfidf.predict(X_test_tfidf)\n",
        "print(\"TF-IDF Best Parameters:\", grid_search_tfidf.best_params_)\n",
        "print(\"TF-IDF Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "# ---- Word2Vec Embedding ----\n",
        "X_train_tokens = X_train.apply(lambda x: word_tokenize(x.lower()))\n",
        "X_test_tokens = X_test.apply(lambda x: word_tokenize(x.lower()))\n",
        "word2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word2vec(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_train_tokens])\n",
        "X_test_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Grid search with Logistic Regression for Word2Vec features\n",
        "logreg_w2v = LogisticRegression()\n",
        "grid_search_w2v = GridSearchCV(logreg_w2v, param_grid, cv=5)\n",
        "grid_search_w2v.fit(X_train_w2v, y_train)\n",
        "y_pred_w2v = grid_search_w2v.predict(X_test_w2v)\n",
        "print(\"Word2Vec Best Parameters:\", grid_search_w2v.best_params_)\n",
        "print(\"Word2Vec Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_w2v))\n",
        "\n",
        "# ---- GloVe Embedding ----\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def average_glove(tokens, model):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_glove = np.array([average_glove(tokens, glove_model) for tokens in X_train_tokens])\n",
        "X_test_glove = np.array([average_glove(tokens, glove_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Grid search with Logistic Regression for GloVe features\n",
        "logreg_glove = LogisticRegression()\n",
        "grid_search_glove = GridSearchCV(logreg_glove, param_grid, cv=5)\n",
        "grid_search_glove.fit(X_train_glove, y_train)\n",
        "y_pred_glove = grid_search_glove.predict(X_test_glove)\n",
        "print(\"GloVe Best Parameters:\", grid_search_glove.best_params_)\n",
        "print(\"GloVe Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_glove))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XHguOdkm9O1"
      },
      "source": [
        "**Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOeZb3pfslxW",
        "outputId": "4eedce8d-34eb-4a39-c3de-bc58ebb16ec2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.81      2996\n",
            "           1       0.77      0.72      0.74      2346\n",
            "\n",
            "    accuracy                           0.78      5342\n",
            "   macro avg       0.78      0.78      0.78      5342\n",
            "weighted avg       0.78      0.78      0.78      5342\n",
            "\n",
            "Word2Vec Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.73      0.77      2996\n",
            "           1       0.69      0.77      0.73      2346\n",
            "\n",
            "    accuracy                           0.75      5342\n",
            "   macro avg       0.75      0.75      0.75      5342\n",
            "weighted avg       0.75      0.75      0.75      5342\n",
            "\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "GloVe Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.82      0.79      2996\n",
            "           1       0.74      0.68      0.71      2346\n",
            "\n",
            "    accuracy                           0.75      5342\n",
            "   macro avg       0.75      0.75      0.75      5342\n",
            "weighted avg       0.75      0.75      0.75      5342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Ensure NLTK tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('/content/Sarcasm_Headlines_Dataset.xlsx')\n",
        "df['headline'] = df['headline'].astype(str)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- TF-IDF Embedding ----\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Limit to 5000 features\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Fit SVM for TF-IDF features with fixed parameters\n",
        "svm_tfidf = SVC(kernel='linear', C=1)  # Use a fixed value for C\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"TF-IDF Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "# ---- Word2Vec Embedding ----\n",
        "X_train_tokens = X_train.apply(lambda x: word_tokenize(x.lower()))\n",
        "X_test_tokens = X_test.apply(lambda x: word_tokenize(x.lower()))\n",
        "word2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word2vec(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_train_tokens])\n",
        "X_test_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Fit SVM for Word2Vec features with fixed parameters\n",
        "svm_w2v = SVC(kernel='linear', C=1)  # Use a fixed value for C\n",
        "svm_w2v.fit(X_train_w2v, y_train)\n",
        "y_pred_w2v = svm_w2v.predict(X_test_w2v)\n",
        "\n",
        "print(\"Word2Vec Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_w2v))\n",
        "\n",
        "# ---- GloVe Embedding ----\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def average_glove(tokens, model):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_glove = np.array([average_glove(tokens, glove_model) for tokens in X_train_tokens])\n",
        "X_test_glove = np.array([average_glove(tokens, glove_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Fit SVM for GloVe features with fixed parameters\n",
        "svm_glove = SVC(kernel='linear', C=1)  # Use a fixed value for C\n",
        "svm_glove.fit(X_train_glove, y_train)\n",
        "y_pred_glove = svm_glove.predict(X_test_glove)\n",
        "\n",
        "print(\"GloVe Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_glove))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQxSQ_nTzHWN"
      },
      "source": [
        "**Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nEW1LHhz12M",
        "outputId": "62877ae9-60f4-485f-98bc-98a8439bc1de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.75      0.75      2996\n",
            "           1       0.68      0.69      0.69      2346\n",
            "\n",
            "    accuracy                           0.72      5342\n",
            "   macro avg       0.72      0.72      0.72      5342\n",
            "weighted avg       0.72      0.72      0.72      5342\n",
            "\n",
            "Word2Vec Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.70      0.69      2996\n",
            "           1       0.61      0.61      0.61      2346\n",
            "\n",
            "    accuracy                           0.66      5342\n",
            "   macro avg       0.65      0.65      0.65      5342\n",
            "weighted avg       0.66      0.66      0.66      5342\n",
            "\n",
            "GloVe Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.67      0.67      2996\n",
            "           1       0.57      0.57      0.57      2346\n",
            "\n",
            "    accuracy                           0.63      5342\n",
            "   macro avg       0.62      0.62      0.62      5342\n",
            "weighted avg       0.63      0.63      0.63      5342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier  # Import Decision Tree\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Ensure NLTK tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('/content/Sarcasm_Headlines_Dataset.xlsx')\n",
        "df['headline'] = df['headline'].astype(str)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- TF-IDF Embedding ----\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Limit to 5000 features\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Fit Decision Tree for TF-IDF features\n",
        "dt_tfidf = DecisionTreeClassifier()  # Initialize Decision Tree Classifier\n",
        "dt_tfidf.fit(X_train_tfidf, y_train)  # Convert sparse matrix to dense\n",
        "y_pred_tfidf = dt_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"TF-IDF Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "# ---- Word2Vec Embedding ----\n",
        "X_train_tokens = X_train.apply(lambda x: word_tokenize(x.lower()))\n",
        "X_test_tokens = X_test.apply(lambda x: word_tokenize(x.lower()))\n",
        "word2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word2vec(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_train_tokens])\n",
        "X_test_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Fit Decision Tree for Word2Vec features\n",
        "dt_w2v = DecisionTreeClassifier()  # Initialize Decision Tree Classifier\n",
        "dt_w2v.fit(X_train_w2v, y_train)\n",
        "y_pred_w2v = dt_w2v.predict(X_test_w2v)\n",
        "\n",
        "print(\"Word2Vec Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_w2v))\n",
        "\n",
        "# ---- GloVe Embedding ----\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def average_glove(tokens, model):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_glove = np.array([average_glove(tokens, glove_model) for tokens in X_train_tokens])\n",
        "X_test_glove = np.array([average_glove(tokens, glove_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Fit Decision Tree for GloVe features\n",
        "dt_glove = DecisionTreeClassifier()  # Initialize Decision Tree Classifier\n",
        "dt_glove.fit(X_train_glove, y_train)\n",
        "y_pred_glove = dt_glove.predict(X_test_glove)\n",
        "\n",
        "print(\"GloVe Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_glove))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V9zUqEE2Ef6"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5WFahJk2IzC",
        "outputId": "ef9b9ccb-ed94-445b-8746-82fc189cd927"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80      2996\n",
            "           1       0.75      0.71      0.73      2346\n",
            "\n",
            "    accuracy                           0.77      5342\n",
            "   macro avg       0.77      0.76      0.77      5342\n",
            "weighted avg       0.77      0.77      0.77      5342\n",
            "\n",
            "Word2Vec Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.77      0.77      2996\n",
            "           1       0.71      0.72      0.71      2346\n",
            "\n",
            "    accuracy                           0.75      5342\n",
            "   macro avg       0.74      0.74      0.74      5342\n",
            "weighted avg       0.75      0.75      0.75      5342\n",
            "\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "GloVe Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.85      0.79      2996\n",
            "           1       0.77      0.62      0.69      2346\n",
            "\n",
            "    accuracy                           0.75      5342\n",
            "   macro avg       0.76      0.74      0.74      5342\n",
            "weighted avg       0.75      0.75      0.75      5342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Ensure NLTK tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('/content/Sarcasm_Headlines_Dataset.xlsx')\n",
        "df['headline'] = df['headline'].astype(str)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['headline'], df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- TF-IDF Embedding ----\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Limit to 5000 features\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Fit Random Forest for TF-IDF features\n",
        "rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)  # Initialize Random Forest Classifier\n",
        "rf_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"TF-IDF Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "# ---- Word2Vec Embedding ----\n",
        "X_train_tokens = X_train.apply(lambda x: word_tokenize(x.lower()))\n",
        "X_test_tokens = X_test.apply(lambda x: word_tokenize(x.lower()))\n",
        "word2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word2vec(tokens, model):\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_train_tokens])\n",
        "X_test_w2v = np.array([average_word2vec(tokens, word2vec_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Fit Random Forest for Word2Vec features\n",
        "rf_w2v = RandomForestClassifier(n_estimators=100, random_state=42)  # Initialize Random Forest Classifier\n",
        "rf_w2v.fit(X_train_w2v, y_train)\n",
        "y_pred_w2v = rf_w2v.predict(X_test_w2v)\n",
        "\n",
        "print(\"Word2Vec Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_w2v))\n",
        "\n",
        "# ---- GloVe Embedding ----\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "def average_glove(tokens, model):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_glove = np.array([average_glove(tokens, glove_model) for tokens in X_train_tokens])\n",
        "X_test_glove = np.array([average_glove(tokens, glove_model) for tokens in X_test_tokens])\n",
        "\n",
        "# Fit Random Forest for GloVe features\n",
        "rf_glove = RandomForestClassifier(n_estimators=100, random_state=42)  # Initialize Random Forest Classifier\n",
        "rf_glove.fit(X_train_glove, y_train)\n",
        "y_pred_glove = rf_glove.predict(X_test_glove)\n",
        "\n",
        "print(\"GloVe Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_glove))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_FZ_hXU3Cai"
      },
      "source": [
        "**Gradient Boost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TivsE40B3ILe",
        "outputId": "0124bb85-ece1-47d7-ba91-a63c9ad9e538"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Sarcasm Detection with Gradient Boosting:\n",
            "\n",
            "TF-IDF Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.67      0.75      2996\n",
            "           1       0.67      0.83      0.74      2346\n",
            "\n",
            "    accuracy                           0.74      5342\n",
            "   macro avg       0.75      0.75      0.74      5342\n",
            "weighted avg       0.76      0.74      0.74      5342\n",
            "\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "GloVe Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.81      0.77      2996\n",
            "           1       0.72      0.62      0.66      2346\n",
            "\n",
            "    accuracy                           0.73      5342\n",
            "   macro avg       0.73      0.71      0.72      5342\n",
            "weighted avg       0.73      0.73      0.72      5342\n",
            "\n",
            "Word2Vec Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.90      0.72      2996\n",
            "           1       0.65      0.25      0.36      2346\n",
            "\n",
            "    accuracy                           0.61      5342\n",
            "   macro avg       0.63      0.57      0.54      5342\n",
            "weighted avg       0.63      0.61      0.56      5342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_excel(\"/content/Sarcasm_Headlines_Dataset.xlsx\")\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "data['tokens'] = data['headline'].apply(preprocess_text)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X = data['headline']\n",
        "y = data['is_sarcastic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize train and test sets\n",
        "X_train_tokens = X_train.apply(preprocess_text)\n",
        "X_test_tokens = X_test.apply(preprocess_text)\n",
        "\n",
        "# TF-IDF method\n",
        "def tfidf_method():\n",
        "    tfidf = TfidfVectorizer()\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "    model = GradientBoostingClassifier()\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    print(\"TF-IDF Classification Report:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# GloVe method\n",
        "def glove_method():\n",
        "    # ---- GloVe Embedding ----\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "    def average_glove(tokens, model):\n",
        "        vectors = [model[word] for word in tokens if word in model]\n",
        "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
        "\n",
        "    X_train_glove = np.array([average_glove(tokens, glove_model) for tokens in X_train_tokens])\n",
        "    X_test_glove = np.array([average_glove(tokens, glove_model) for tokens in X_test_tokens])\n",
        "\n",
        "    model = GradientBoostingClassifier()\n",
        "    model.fit(X_train_glove, y_train)\n",
        "    y_pred = model.predict(X_test_glove)\n",
        "\n",
        "    print(\"GloVe Classification Report:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Word2Vec method\n",
        "def word2vec_method():\n",
        "    w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    def get_embedding(tokens):\n",
        "        embeddings = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
        "        return np.mean(embeddings, axis=0) if embeddings else np.zeros(100)\n",
        "\n",
        "    X_train_w2v = np.array([get_embedding(tokens) for tokens in X_train_tokens])\n",
        "    X_test_w2v = np.array([get_embedding(tokens) for tokens in X_test_tokens])\n",
        "\n",
        "    model = GradientBoostingClassifier()\n",
        "    model.fit(X_train_w2v, y_train)\n",
        "    y_pred = model.predict(X_test_w2v)\n",
        "\n",
        "    print(\"Word2Vec Classification Report:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Run all methods\n",
        "print(\"Running Sarcasm Detection with Gradient Boosting:\\n\")\n",
        "tfidf_method()\n",
        "glove_method()\n",
        "word2vec_method()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLxcCF9Xya7s"
      },
      "source": [
        "**RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z5t01xfb2dH_",
        "outputId": "c99c23f4-c437-4b5f-9f4b-ff8554754515"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running TF-IDF Method:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6739 - loss: 0.5726 - val_accuracy: 0.8334 - val_loss: 0.3772\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8945 - loss: 0.2680 - val_accuracy: 0.8318 - val_loss: 0.3898\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9320 - loss: 0.1897 - val_accuracy: 0.8271 - val_loss: 0.4427\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9567 - loss: 0.1252 - val_accuracy: 0.8219 - val_loss: 0.5328\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0688 - val_accuracy: 0.8196 - val_loss: 0.6446\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0337 - val_accuracy: 0.8173 - val_loss: 0.7855\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0146 - val_accuracy: 0.8161 - val_loss: 0.8721\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0108 - val_accuracy: 0.8177 - val_loss: 1.0248\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0056 - val_accuracy: 0.8168 - val_loss: 1.1018\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 0.8168 - val_loss: 1.1624\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0045 - val_accuracy: 0.8149 - val_loss: 1.2156\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0025 - val_accuracy: 0.8114 - val_loss: 1.2257\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 0.8145 - val_loss: 1.3064\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 0.8180 - val_loss: 1.3655\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 0.8173 - val_loss: 1.4088\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.8168 - val_loss: 1.4430\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 0.8177 - val_loss: 1.4673\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0021 - val_accuracy: 0.8166 - val_loss: 1.4891\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.8175 - val_loss: 1.5113\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0023 - val_accuracy: 0.8131 - val_loss: 1.4605\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 0.8163 - val_loss: 1.5021\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0019 - val_accuracy: 0.8180 - val_loss: 1.5652\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.8182 - val_loss: 1.5996\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.8180 - val_loss: 1.6539\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0027 - val_accuracy: 0.8173 - val_loss: 1.6476\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0028 - val_accuracy: 0.8140 - val_loss: 1.5790\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.8175 - val_loss: 1.6607\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0014 - val_accuracy: 0.8191 - val_loss: 1.7103\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9991 - loss: 0.0025 - val_accuracy: 0.8180 - val_loss: 1.6291\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0011 - val_accuracy: 0.8189 - val_loss: 1.7074\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0014 - val_accuracy: 0.8187 - val_loss: 1.7286\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.8161 - val_loss: 1.7786\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.8159 - val_loss: 1.8309\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0021 - val_accuracy: 0.8191 - val_loss: 1.7472\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0017 - val_accuracy: 0.8177 - val_loss: 1.7989\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.8196 - val_loss: 1.8440\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0016 - val_accuracy: 0.8196 - val_loss: 1.8786\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0015 - val_accuracy: 0.8156 - val_loss: 1.9193\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 9.9154e-04 - val_accuracy: 0.8161 - val_loss: 1.9187\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.8166 - val_loss: 1.9264\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0011 - val_accuracy: 0.8170 - val_loss: 1.8769\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 0.8147 - val_loss: 1.9378\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0023 - val_accuracy: 0.8187 - val_loss: 1.9874\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0016 - val_accuracy: 0.8152 - val_loss: 1.9660\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9993 - loss: 0.0016 - val_accuracy: 0.8180 - val_loss: 1.9962\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.8175 - val_loss: 2.0105\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 0.8175 - val_loss: 2.0250\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 0.8191 - val_loss: 2.0931\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 0.8149 - val_loss: 2.0593\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 9.8000e-04 - val_accuracy: 0.8187 - val_loss: 2.2596\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "TF-IDF Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84      2996\n",
            "           1       0.80      0.77      0.79      2346\n",
            "\n",
            "    accuracy                           0.82      5342\n",
            "   macro avg       0.81      0.81      0.81      5342\n",
            "weighted avg       0.82      0.82      0.82      5342\n",
            "\n",
            "\n",
            "Running GloVe Method:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,653,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d_3                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m2,653,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d_3                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_9 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,653,600</span> (10.12 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,653,600\u001b[0m (10.12 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,653,600</span> (10.12 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,653,600\u001b[0m (10.12 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.5410 - loss: 0.6932 - val_accuracy: 0.5538 - val_loss: 0.6910\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5444 - loss: 0.6923 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5591 - loss: 0.6873 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5609 - loss: 0.6878 - val_accuracy: 0.5533 - val_loss: 0.6934\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5561 - loss: 0.6882 - val_accuracy: 0.5538 - val_loss: 0.6881\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5641 - loss: 0.6866 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5621 - loss: 0.6872 - val_accuracy: 0.5538 - val_loss: 0.6885\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5572 - loss: 0.6870 - val_accuracy: 0.5540 - val_loss: 0.6957\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5569 - loss: 0.6877 - val_accuracy: 0.5540 - val_loss: 0.6896\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5654 - loss: 0.6855 - val_accuracy: 0.5555 - val_loss: 0.6896\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5641 - loss: 0.6854 - val_accuracy: 0.5538 - val_loss: 0.6887\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5612 - loss: 0.6875 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5561 - loss: 0.6870 - val_accuracy: 0.5538 - val_loss: 0.6881\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5565 - loss: 0.6872 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5719 - loss: 0.6837 - val_accuracy: 0.5538 - val_loss: 0.6885\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5621 - loss: 0.6862 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5701 - loss: 0.6834 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5531 - loss: 0.6874 - val_accuracy: 0.5552 - val_loss: 0.6893\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5617 - loss: 0.6864 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5610 - loss: 0.6865 - val_accuracy: 0.5538 - val_loss: 0.6913\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5661 - loss: 0.6855 - val_accuracy: 0.5550 - val_loss: 0.6881\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5586 - loss: 0.6874 - val_accuracy: 0.5550 - val_loss: 0.6889\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5665 - loss: 0.6865 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5547 - loss: 0.6871 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5608 - loss: 0.6869 - val_accuracy: 0.5538 - val_loss: 0.6940\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5606 - loss: 0.6878 - val_accuracy: 0.5540 - val_loss: 0.6881\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5659 - loss: 0.6847 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.5669 - loss: 0.6849 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5639 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5614 - loss: 0.6864 - val_accuracy: 0.5540 - val_loss: 0.6879\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5602 - loss: 0.6863 - val_accuracy: 0.5543 - val_loss: 0.6910\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5614 - loss: 0.6876 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5666 - loss: 0.6853 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5525 - loss: 0.6869 - val_accuracy: 0.5550 - val_loss: 0.6877\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5652 - loss: 0.6849 - val_accuracy: 0.5543 - val_loss: 0.6877\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5605 - loss: 0.6865 - val_accuracy: 0.5545 - val_loss: 0.6934\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5504 - loss: 0.6880 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.5521 - loss: 0.6885 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5701 - loss: 0.6849 - val_accuracy: 0.5538 - val_loss: 0.6894\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5626 - loss: 0.6863 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5595 - loss: 0.6876 - val_accuracy: 0.5538 - val_loss: 0.6921\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5590 - loss: 0.6887 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5667 - loss: 0.6858 - val_accuracy: 0.5538 - val_loss: 0.6870\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5686 - loss: 0.6844 - val_accuracy: 0.5538 - val_loss: 0.6869\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5579 - loss: 0.6874 - val_accuracy: 0.5538 - val_loss: 0.6908\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5575 - loss: 0.6882 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5587 - loss: 0.6873 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5508 - loss: 0.6877 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5611 - loss: 0.6869 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.5646 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6887\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "GloVe Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72      2996\n",
            "           1       0.00      0.00      0.00      2346\n",
            "\n",
            "    accuracy                           0.56      5342\n",
            "   macro avg       0.28      0.50      0.36      5342\n",
            "weighted avg       0.31      0.56      0.40      5342\n",
            "\n",
            "\n",
            "Running Word2Vec Method:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,653,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d_4                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m2,653,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ spatial_dropout1d_4                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_10 (\u001b[38;5;33mSimpleRNN\u001b[0m)            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,653,600</span> (10.12 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,653,600\u001b[0m (10.12 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,653,600</span> (10.12 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,653,600\u001b[0m (10.12 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.5407 - loss: 0.6921 - val_accuracy: 0.5538 - val_loss: 0.6866\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.5379 - loss: 0.6949 - val_accuracy: 0.5648 - val_loss: 0.6817\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6032 - loss: 0.6562 - val_accuracy: 0.5952 - val_loss: 0.6623\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6879 - loss: 0.5860 - val_accuracy: 0.7239 - val_loss: 0.5666\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8013 - loss: 0.4604 - val_accuracy: 0.7309 - val_loss: 0.5833\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8391 - loss: 0.3950 - val_accuracy: 0.7291 - val_loss: 0.5956\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8378 - loss: 0.3929 - val_accuracy: 0.7263 - val_loss: 0.6287\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8390 - loss: 0.3947 - val_accuracy: 0.7260 - val_loss: 0.6198\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8366 - loss: 0.3962 - val_accuracy: 0.7038 - val_loss: 0.6366\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8168 - loss: 0.4204 - val_accuracy: 0.6979 - val_loss: 0.6331\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.8131 - loss: 0.4299 - val_accuracy: 0.6982 - val_loss: 0.6290\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8160 - loss: 0.4265 - val_accuracy: 0.7045 - val_loss: 0.6170\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8092 - loss: 0.4392 - val_accuracy: 0.6982 - val_loss: 0.6178\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8160 - loss: 0.4234 - val_accuracy: 0.6984 - val_loss: 0.6206\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8189 - loss: 0.4264 - val_accuracy: 0.7033 - val_loss: 0.6224\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8070 - loss: 0.4367 - val_accuracy: 0.6972 - val_loss: 0.6206\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8149 - loss: 0.4272 - val_accuracy: 0.7036 - val_loss: 0.6211\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.8081 - loss: 0.4404 - val_accuracy: 0.6958 - val_loss: 0.6436\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.8087 - loss: 0.4393 - val_accuracy: 0.7026 - val_loss: 0.6154\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8189 - loss: 0.4249 - val_accuracy: 0.6963 - val_loss: 0.6426\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8015 - loss: 0.4401 - val_accuracy: 0.6954 - val_loss: 0.6232\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8110 - loss: 0.4323 - val_accuracy: 0.6951 - val_loss: 0.6298\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8121 - loss: 0.4340 - val_accuracy: 0.6968 - val_loss: 0.6307\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8140 - loss: 0.4279 - val_accuracy: 0.7022 - val_loss: 0.6243\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8134 - loss: 0.4297 - val_accuracy: 0.7087 - val_loss: 0.6253\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8202 - loss: 0.4230 - val_accuracy: 0.6989 - val_loss: 0.6273\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8142 - loss: 0.4259 - val_accuracy: 0.7057 - val_loss: 0.6300\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8121 - loss: 0.4343 - val_accuracy: 0.7015 - val_loss: 0.6361\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8039 - loss: 0.4609 - val_accuracy: 0.6975 - val_loss: 0.6280\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8140 - loss: 0.4513 - val_accuracy: 0.7000 - val_loss: 0.6172\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8114 - loss: 0.4535 - val_accuracy: 0.7029 - val_loss: 0.6335\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8137 - loss: 0.4484 - val_accuracy: 0.7026 - val_loss: 0.6234\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8090 - loss: 0.4506 - val_accuracy: 0.7029 - val_loss: 0.6411\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8214 - loss: 0.4345 - val_accuracy: 0.7040 - val_loss: 0.6131\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.8054 - loss: 0.4569 - val_accuracy: 0.7033 - val_loss: 0.6099\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8146 - loss: 0.4421 - val_accuracy: 0.7017 - val_loss: 0.6092\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8126 - loss: 0.4494 - val_accuracy: 0.7047 - val_loss: 0.6326\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8147 - loss: 0.4458 - val_accuracy: 0.7057 - val_loss: 0.6176\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8153 - loss: 0.4402 - val_accuracy: 0.7059 - val_loss: 0.6218\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8141 - loss: 0.4451 - val_accuracy: 0.7047 - val_loss: 0.6166\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8126 - loss: 0.4422 - val_accuracy: 0.7066 - val_loss: 0.6043\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8155 - loss: 0.4368 - val_accuracy: 0.7075 - val_loss: 0.6257\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8106 - loss: 0.4460 - val_accuracy: 0.7075 - val_loss: 0.6267\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8140 - loss: 0.4371 - val_accuracy: 0.7059 - val_loss: 0.6011\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.8113 - loss: 0.4390 - val_accuracy: 0.7047 - val_loss: 0.6090\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.8156 - loss: 0.4417 - val_accuracy: 0.7061 - val_loss: 0.5981\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.8112 - loss: 0.4440 - val_accuracy: 0.7057 - val_loss: 0.6320\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8158 - loss: 0.4419 - val_accuracy: 0.7059 - val_loss: 0.6258\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8125 - loss: 0.4450 - val_accuracy: 0.7057 - val_loss: 0.6083\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.8216 - loss: 0.4335 - val_accuracy: 0.7089 - val_loss: 0.6107\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "Word2Vec Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.69      0.72      2996\n",
            "           1       0.65      0.71      0.68      2346\n",
            "\n",
            "    accuracy                           0.70      5342\n",
            "   macro avg       0.70      0.70      0.70      5342\n",
            "weighted avg       0.71      0.70      0.70      5342\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_excel(\"/content/Sarcasm_Headlines_Dataset.xlsx\")\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "data['tokens'] = data['headline'].apply(preprocess_text)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X = data['headline']\n",
        "y = data['is_sarcastic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize train and test sets\n",
        "X_train_tokens = X_train.apply(preprocess_text)\n",
        "X_test_tokens = X_test.apply(preprocess_text)\n",
        "\n",
        "# Tokenizer for text sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences\n",
        "max_sequence_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 1. TF-IDF Method\n",
        "def tfidf_method():\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_tfidf, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "    y_pred = (model.predict(X_test_tfidf) > 0.5).astype(\"int32\")\n",
        "    print(\"TF-IDF Classification Report:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 2. GloVe Embeddings\n",
        "def create_glove_embedding_matrix():\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "    embedding_dim = 100\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in glove_model:\n",
        "            embedding_matrix[i] = glove_model[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "def glove_rnn_model():\n",
        "    embedding_matrix = create_glove_embedding_matrix()\n",
        "    embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "                  input_length=max_sequence_length, trainable=False),\n",
        "        SpatialDropout1D(0.2),\n",
        "        SimpleRNN(128, dropout=0.2, return_sequences=False),  # SimpleRNN expects 3D input\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. Word2Vec Embeddings\n",
        "def create_word2vec_embedding_matrix():\n",
        "    w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    embedding_dim = w2v_model.vector_size\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = w2v_model.wv[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "def word2vec_rnn_model():\n",
        "    embedding_matrix = create_word2vec_embedding_matrix()\n",
        "    embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "                  input_length=max_sequence_length, trainable=True),\n",
        "        SpatialDropout1D(0.2),\n",
        "        SimpleRNN(128, dropout=0.2, return_sequences=False),  # SimpleRNN expects 3D input\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and Evaluate Models\n",
        "def train_and_evaluate_rnn(method):\n",
        "    if method == \"TF-IDF\":\n",
        "        tfidf_method()\n",
        "    elif method == \"GloVe\":\n",
        "        model = glove_rnn_model()\n",
        "        print(model.summary())\n",
        "        model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "        y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "        print(\"GloVe Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "    elif method == \"Word2Vec\":\n",
        "        model = word2vec_rnn_model()\n",
        "        print(model.summary())\n",
        "        model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "        y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "        print(\"Word2Vec Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "    else:\n",
        "        print(\"Invalid method. Choose from 'TF-IDF', 'GloVe', or 'Word2Vec'.\")\n",
        "\n",
        "# Run all methods\n",
        "methods = [\"TF-IDF\", \"GloVe\", \"Word2Vec\"]\n",
        "for method in methods:\n",
        "    print(f\"\\nRunning {method} Method:\\n\")\n",
        "    train_and_evaluate_rnn(method)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otgSr473ygL4"
      },
      "source": [
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEeGsDqx_dKB",
        "outputId": "1765ca90-2172-41cb-dee1-eaa41b91168a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running TF-IDF Method:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.6162 - loss: 0.6406 - val_accuracy: 0.8262 - val_loss: 0.4198\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8426 - loss: 0.3789 - val_accuracy: 0.8367 - val_loss: 0.3731\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8588 - loss: 0.3186 - val_accuracy: 0.8339 - val_loss: 0.3767\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8680 - loss: 0.2994 - val_accuracy: 0.8348 - val_loss: 0.3863\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8822 - loss: 0.2769 - val_accuracy: 0.8252 - val_loss: 0.3978\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.8767 - loss: 0.2773 - val_accuracy: 0.8238 - val_loss: 0.4101\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8823 - loss: 0.2668 - val_accuracy: 0.8191 - val_loss: 0.4193\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8889 - loss: 0.2536 - val_accuracy: 0.8229 - val_loss: 0.4267\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8893 - loss: 0.2565 - val_accuracy: 0.8219 - val_loss: 0.4314\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8819 - loss: 0.2643 - val_accuracy: 0.8229 - val_loss: 0.4391\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8838 - loss: 0.2595 - val_accuracy: 0.8205 - val_loss: 0.4409\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8827 - loss: 0.2616 - val_accuracy: 0.8210 - val_loss: 0.4406\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8898 - loss: 0.2441 - val_accuracy: 0.8203 - val_loss: 0.4448\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8923 - loss: 0.2463 - val_accuracy: 0.8234 - val_loss: 0.4479\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8924 - loss: 0.2406 - val_accuracy: 0.8187 - val_loss: 0.4559\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8931 - loss: 0.2428 - val_accuracy: 0.8170 - val_loss: 0.4662\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8888 - loss: 0.2405 - val_accuracy: 0.8161 - val_loss: 0.4640\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8960 - loss: 0.2353 - val_accuracy: 0.8184 - val_loss: 0.4634\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8976 - loss: 0.2349 - val_accuracy: 0.8154 - val_loss: 0.4728\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8924 - loss: 0.2317 - val_accuracy: 0.8159 - val_loss: 0.4724\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.8984 - loss: 0.2302 - val_accuracy: 0.8156 - val_loss: 0.4784\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8968 - loss: 0.2288 - val_accuracy: 0.8161 - val_loss: 0.4856\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9021 - loss: 0.2259 - val_accuracy: 0.8149 - val_loss: 0.4910\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9003 - loss: 0.2226 - val_accuracy: 0.8142 - val_loss: 0.4930\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9019 - loss: 0.2167 - val_accuracy: 0.8170 - val_loss: 0.4996\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9055 - loss: 0.2157 - val_accuracy: 0.8152 - val_loss: 0.5042\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9051 - loss: 0.2149 - val_accuracy: 0.8184 - val_loss: 0.5089\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9048 - loss: 0.2118 - val_accuracy: 0.8182 - val_loss: 0.5196\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9123 - loss: 0.2001 - val_accuracy: 0.8175 - val_loss: 0.5236\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9097 - loss: 0.2010 - val_accuracy: 0.8177 - val_loss: 0.5370\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9095 - loss: 0.2036 - val_accuracy: 0.8187 - val_loss: 0.5360\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9175 - loss: 0.1871 - val_accuracy: 0.8147 - val_loss: 0.5517\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9186 - loss: 0.1815 - val_accuracy: 0.8156 - val_loss: 0.5567\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9206 - loss: 0.1795 - val_accuracy: 0.8166 - val_loss: 0.5722\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9321 - loss: 0.1664 - val_accuracy: 0.8166 - val_loss: 0.5795\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9261 - loss: 0.1681 - val_accuracy: 0.8152 - val_loss: 0.5830\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9359 - loss: 0.1520 - val_accuracy: 0.8140 - val_loss: 0.6032\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9360 - loss: 0.1519 - val_accuracy: 0.8163 - val_loss: 0.6010\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9378 - loss: 0.1440 - val_accuracy: 0.8170 - val_loss: 0.6210\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9371 - loss: 0.1463 - val_accuracy: 0.8194 - val_loss: 0.6219\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9470 - loss: 0.1323 - val_accuracy: 0.8161 - val_loss: 0.6481\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9492 - loss: 0.1284 - val_accuracy: 0.8145 - val_loss: 0.6479\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9523 - loss: 0.1176 - val_accuracy: 0.8208 - val_loss: 0.6569\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9495 - loss: 0.1263 - val_accuracy: 0.8154 - val_loss: 0.6668\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9576 - loss: 0.1060 - val_accuracy: 0.8191 - val_loss: 0.6691\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9568 - loss: 0.1093 - val_accuracy: 0.8175 - val_loss: 0.6774\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9572 - loss: 0.1091 - val_accuracy: 0.8152 - val_loss: 0.6928\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9606 - loss: 0.1072 - val_accuracy: 0.8161 - val_loss: 0.6972\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9635 - loss: 0.0964 - val_accuracy: 0.8184 - val_loss: 0.7061\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9652 - loss: 0.0902 - val_accuracy: 0.8177 - val_loss: 0.7128\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "TF-IDF Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84      2996\n",
            "           1       0.81      0.78      0.79      2346\n",
            "\n",
            "    accuracy                           0.82      5342\n",
            "   macro avg       0.82      0.82      0.82      5342\n",
            "weighted avg       0.82      0.82      0.82      5342\n",
            "\n",
            "\n",
            "Running GloVe Method:\n",
            "\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 162ms/step - accuracy: 0.5603 - loss: 0.6870 - val_accuracy: 0.5538 - val_loss: 0.6882\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 176ms/step - accuracy: 0.5625 - loss: 0.6858 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 163ms/step - accuracy: 0.5574 - loss: 0.6869 - val_accuracy: 0.5538 - val_loss: 0.6892\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 165ms/step - accuracy: 0.5628 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 159ms/step - accuracy: 0.5582 - loss: 0.6866 - val_accuracy: 0.5538 - val_loss: 0.6895\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 157ms/step - accuracy: 0.5700 - loss: 0.6833 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 164ms/step - accuracy: 0.5712 - loss: 0.6832 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 161ms/step - accuracy: 0.5631 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 159ms/step - accuracy: 0.5654 - loss: 0.6849 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 162ms/step - accuracy: 0.5608 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 159ms/step - accuracy: 0.5615 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 160ms/step - accuracy: 0.5618 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 168ms/step - accuracy: 0.5647 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 160ms/step - accuracy: 0.5664 - loss: 0.6845 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 165ms/step - accuracy: 0.5609 - loss: 0.6860 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 160ms/step - accuracy: 0.5624 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 158ms/step - accuracy: 0.5627 - loss: 0.6853 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 164ms/step - accuracy: 0.5584 - loss: 0.6865 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 165ms/step - accuracy: 0.5658 - loss: 0.6846 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 164ms/step - accuracy: 0.5616 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 163ms/step - accuracy: 0.5640 - loss: 0.6851 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 160ms/step - accuracy: 0.5618 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 169ms/step - accuracy: 0.5604 - loss: 0.6861 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 166ms/step - accuracy: 0.5638 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 170ms/step - accuracy: 0.5616 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 164ms/step - accuracy: 0.5531 - loss: 0.6877 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 164ms/step - accuracy: 0.5581 - loss: 0.6867 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 160ms/step - accuracy: 0.5668 - loss: 0.6844 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 159ms/step - accuracy: 0.5654 - loss: 0.6847 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 161ms/step - accuracy: 0.5542 - loss: 0.6873 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 159ms/step - accuracy: 0.5664 - loss: 0.6842 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 162ms/step - accuracy: 0.5624 - loss: 0.6854 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 163ms/step - accuracy: 0.5646 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 170ms/step - accuracy: 0.5626 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 160ms/step - accuracy: 0.5684 - loss: 0.6840 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 165ms/step - accuracy: 0.5665 - loss: 0.6844 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 160ms/step - accuracy: 0.5573 - loss: 0.6866 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 166ms/step - accuracy: 0.5622 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 172ms/step - accuracy: 0.5580 - loss: 0.6866 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 158ms/step - accuracy: 0.5640 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 160ms/step - accuracy: 0.5610 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 159ms/step - accuracy: 0.5649 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 163ms/step - accuracy: 0.5614 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 159ms/step - accuracy: 0.5593 - loss: 0.6861 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 158ms/step - accuracy: 0.5611 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 161ms/step - accuracy: 0.5649 - loss: 0.6848 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 160ms/step - accuracy: 0.5619 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 163ms/step - accuracy: 0.5588 - loss: 0.6863 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 161ms/step - accuracy: 0.5654 - loss: 0.6847 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 159ms/step - accuracy: 0.5662 - loss: 0.6844 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 47ms/step\n",
            "GloVe Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72      2996\n",
            "           1       0.00      0.00      0.00      2346\n",
            "\n",
            "    accuracy                           0.56      5342\n",
            "   macro avg       0.28      0.50      0.36      5342\n",
            "weighted avg       0.31      0.56      0.40      5342\n",
            "\n",
            "\n",
            "Running Word2Vec Method:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 186ms/step - accuracy: 0.5651 - loss: 0.6869 - val_accuracy: 0.5538 - val_loss: 0.6880\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 186ms/step - accuracy: 0.5655 - loss: 0.6851 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5558 - loss: 0.6873 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5643 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 181ms/step - accuracy: 0.5587 - loss: 0.6865 - val_accuracy: 0.5538 - val_loss: 0.6891\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 188ms/step - accuracy: 0.5620 - loss: 0.6858 - val_accuracy: 0.5538 - val_loss: 0.6880\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 185ms/step - accuracy: 0.5590 - loss: 0.6866 - val_accuracy: 0.5538 - val_loss: 0.6880\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 187ms/step - accuracy: 0.5686 - loss: 0.6837 - val_accuracy: 0.5538 - val_loss: 0.6880\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 187ms/step - accuracy: 0.5582 - loss: 0.6869 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 182ms/step - accuracy: 0.5667 - loss: 0.6845 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 186ms/step - accuracy: 0.5665 - loss: 0.6846 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 188ms/step - accuracy: 0.5694 - loss: 0.6837 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 181ms/step - accuracy: 0.5573 - loss: 0.6870 - val_accuracy: 0.5538 - val_loss: 0.6888\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 187ms/step - accuracy: 0.5674 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 188ms/step - accuracy: 0.5638 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 181ms/step - accuracy: 0.5702 - loss: 0.6837 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 185ms/step - accuracy: 0.5561 - loss: 0.6870 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 187ms/step - accuracy: 0.5632 - loss: 0.6853 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 181ms/step - accuracy: 0.5617 - loss: 0.6858 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 186ms/step - accuracy: 0.5630 - loss: 0.6854 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 182ms/step - accuracy: 0.5656 - loss: 0.6847 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 187ms/step - accuracy: 0.5662 - loss: 0.6844 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 186ms/step - accuracy: 0.5614 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 180ms/step - accuracy: 0.5576 - loss: 0.6866 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 185ms/step - accuracy: 0.5636 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 184ms/step - accuracy: 0.5638 - loss: 0.6851 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 182ms/step - accuracy: 0.5649 - loss: 0.6849 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 185ms/step - accuracy: 0.5629 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 180ms/step - accuracy: 0.5578 - loss: 0.6865 - val_accuracy: 0.5538 - val_loss: 0.6881\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 187ms/step - accuracy: 0.5660 - loss: 0.6845 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 185ms/step - accuracy: 0.5634 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 182ms/step - accuracy: 0.5589 - loss: 0.6864 - val_accuracy: 0.5538 - val_loss: 0.6882\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 185ms/step - accuracy: 0.5656 - loss: 0.6847 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5658 - loss: 0.6846 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 179ms/step - accuracy: 0.5635 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 187ms/step - accuracy: 0.5575 - loss: 0.6867 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5629 - loss: 0.6853 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5626 - loss: 0.6853 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5594 - loss: 0.6862 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 183ms/step - accuracy: 0.5620 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5647 - loss: 0.6848 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 179ms/step - accuracy: 0.5599 - loss: 0.6860 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 186ms/step - accuracy: 0.5613 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 186ms/step - accuracy: 0.5680 - loss: 0.6840 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 188ms/step - accuracy: 0.5598 - loss: 0.6861 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 184ms/step - accuracy: 0.5659 - loss: 0.6845 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5565 - loss: 0.6868 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 185ms/step - accuracy: 0.5611 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 184ms/step - accuracy: 0.5619 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 186ms/step - accuracy: 0.5612 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
            "Word2Vec Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72      2996\n",
            "           1       0.00      0.00      0.00      2346\n",
            "\n",
            "    accuracy                           0.56      5342\n",
            "   macro avg       0.28      0.50      0.36      5342\n",
            "weighted avg       0.31      0.56      0.40      5342\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset from Excel file\n",
        "data = pd.read_excel(\"Sarcasm_Headlines_Dataset.xlsx\")\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "data['tokens'] = data['headline'].apply(preprocess_text)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X = data['headline']\n",
        "y = data['is_sarcastic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize train and test sets\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences\n",
        "max_sequence_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 1. TF-IDF with LSTM\n",
        "def tfidf_lstm_model():\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    # Reshape TF-IDF output to 3D for LSTM (samples, timesteps, features)\n",
        "    X_train_tfidf = np.expand_dims(X_train_tfidf, axis=1)\n",
        "    X_test_tfidf = np.expand_dims(X_test_tfidf, axis=1)\n",
        "\n",
        "    model = Sequential([\n",
        "        LSTM(128, input_shape=(X_train_tfidf.shape[1], X_train_tfidf.shape[2]), dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_tfidf, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "    y_pred = (model.predict(X_test_tfidf) > 0.5).astype(\"int32\")\n",
        "    print(\"TF-IDF Classification Report:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 2. GloVe Embeddings\n",
        "def create_glove_embedding_matrix():\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "    embedding_dim = 100\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in glove_model:\n",
        "            embedding_matrix[i] = glove_model[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "def glove_lstm_model():\n",
        "    embedding_matrix = create_glove_embedding_matrix()\n",
        "    embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "                  input_length=max_sequence_length, trainable=False),\n",
        "        SpatialDropout1D(0.2),\n",
        "        LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. Word2Vec Embeddings\n",
        "def create_word2vec_embedding_matrix():\n",
        "    w2v_model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "    embedding_dim = w2v_model.vector_size\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = w2v_model.wv[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "def word2vec_lstm_model():\n",
        "    embedding_matrix = create_word2vec_embedding_matrix()\n",
        "    embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "                  input_length=max_sequence_length, trainable=True),\n",
        "        SpatialDropout1D(0.2),\n",
        "        LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and Evaluate Models\n",
        "def train_and_evaluate_lstm(method):\n",
        "    if method == \"TF-IDF\":\n",
        "        tfidf_lstm_model()\n",
        "    elif method == \"GloVe\":\n",
        "        model = glove_lstm_model()\n",
        "        model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "        y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "        print(\"GloVe Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "    elif method == \"Word2Vec\":\n",
        "        model = word2vec_lstm_model()\n",
        "        model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "        y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "        print(\"Word2Vec Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "    else:\n",
        "        print(\"Invalid method. Choose from 'TF-IDF', 'GloVe', or 'Word2Vec'.\")\n",
        "\n",
        "# Run all methods\n",
        "methods = [\"TF-IDF\", \"GloVe\", \"Word2Vec\"]\n",
        "for method in methods:\n",
        "    print(f\"\\nRunning {method} Method:\\n\")\n",
        "    train_and_evaluate_lstm(method)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_wv-0qEylG2"
      },
      "source": [
        "**GRU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beb2iExnglZy",
        "outputId": "4a6c8068-cd14-4a1b-b57d-4286148b9d33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running TF-IDF Method with GRU:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 47ms/step - accuracy: 0.6325 - loss: 0.6203 - val_accuracy: 0.8350 - val_loss: 0.3917\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.8517 - loss: 0.3516 - val_accuracy: 0.8311 - val_loss: 0.3706\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.8667 - loss: 0.3125 - val_accuracy: 0.8248 - val_loss: 0.3857\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.8738 - loss: 0.2880 - val_accuracy: 0.8252 - val_loss: 0.3922\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.8740 - loss: 0.2942 - val_accuracy: 0.8234 - val_loss: 0.4039\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.8864 - loss: 0.2726 - val_accuracy: 0.8250 - val_loss: 0.4113\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.8842 - loss: 0.2729 - val_accuracy: 0.8215 - val_loss: 0.4174\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 52ms/step - accuracy: 0.8816 - loss: 0.2694 - val_accuracy: 0.8222 - val_loss: 0.4211\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.8857 - loss: 0.2656 - val_accuracy: 0.8241 - val_loss: 0.4217\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.8815 - loss: 0.2648 - val_accuracy: 0.8222 - val_loss: 0.4324\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.8856 - loss: 0.2559 - val_accuracy: 0.8222 - val_loss: 0.4393\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.8877 - loss: 0.2532 - val_accuracy: 0.8187 - val_loss: 0.4345\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - accuracy: 0.8842 - loss: 0.2592 - val_accuracy: 0.8180 - val_loss: 0.4377\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.8893 - loss: 0.2505 - val_accuracy: 0.8238 - val_loss: 0.4475\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 43ms/step - accuracy: 0.8911 - loss: 0.2518 - val_accuracy: 0.8159 - val_loss: 0.4494\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.8915 - loss: 0.2465 - val_accuracy: 0.8173 - val_loss: 0.4470\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.8922 - loss: 0.2494 - val_accuracy: 0.8212 - val_loss: 0.4578\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.8914 - loss: 0.2481 - val_accuracy: 0.8189 - val_loss: 0.4647\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.8888 - loss: 0.2450 - val_accuracy: 0.8189 - val_loss: 0.4618\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 44ms/step - accuracy: 0.8865 - loss: 0.2531 - val_accuracy: 0.8175 - val_loss: 0.4673\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.8876 - loss: 0.2450 - val_accuracy: 0.8205 - val_loss: 0.4727\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 40ms/step - accuracy: 0.8935 - loss: 0.2385 - val_accuracy: 0.8184 - val_loss: 0.4806\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 44ms/step - accuracy: 0.8960 - loss: 0.2333 - val_accuracy: 0.8154 - val_loss: 0.4803\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.8984 - loss: 0.2228 - val_accuracy: 0.8152 - val_loss: 0.4864\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.8964 - loss: 0.2292 - val_accuracy: 0.8173 - val_loss: 0.4845\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.9008 - loss: 0.2228 - val_accuracy: 0.8184 - val_loss: 0.4870\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.9038 - loss: 0.2220 - val_accuracy: 0.8182 - val_loss: 0.4960\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 44ms/step - accuracy: 0.8976 - loss: 0.2317 - val_accuracy: 0.8166 - val_loss: 0.4997\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9004 - loss: 0.2226 - val_accuracy: 0.8170 - val_loss: 0.5068\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 45ms/step - accuracy: 0.9012 - loss: 0.2208 - val_accuracy: 0.8189 - val_loss: 0.5069\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9049 - loss: 0.2176 - val_accuracy: 0.8142 - val_loss: 0.5208\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9041 - loss: 0.2155 - val_accuracy: 0.8184 - val_loss: 0.5237\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.9006 - loss: 0.2163 - val_accuracy: 0.8182 - val_loss: 0.5200\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9069 - loss: 0.2131 - val_accuracy: 0.8219 - val_loss: 0.5276\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.9093 - loss: 0.2066 - val_accuracy: 0.8194 - val_loss: 0.5414\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9114 - loss: 0.2013 - val_accuracy: 0.8208 - val_loss: 0.5397\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.9145 - loss: 0.1962 - val_accuracy: 0.8182 - val_loss: 0.5534\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.9158 - loss: 0.1919 - val_accuracy: 0.8145 - val_loss: 0.5616\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 45ms/step - accuracy: 0.9151 - loss: 0.1875 - val_accuracy: 0.8135 - val_loss: 0.5735\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 40ms/step - accuracy: 0.9182 - loss: 0.1880 - val_accuracy: 0.8180 - val_loss: 0.5794\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 45ms/step - accuracy: 0.9190 - loss: 0.1826 - val_accuracy: 0.8173 - val_loss: 0.5912\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 41ms/step - accuracy: 0.9258 - loss: 0.1748 - val_accuracy: 0.8159 - val_loss: 0.6084\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.9320 - loss: 0.1649 - val_accuracy: 0.8173 - val_loss: 0.6090\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9307 - loss: 0.1649 - val_accuracy: 0.8168 - val_loss: 0.6149\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.9388 - loss: 0.1552 - val_accuracy: 0.8156 - val_loss: 0.6221\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9375 - loss: 0.1489 - val_accuracy: 0.8161 - val_loss: 0.6348\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - accuracy: 0.9355 - loss: 0.1474 - val_accuracy: 0.8147 - val_loss: 0.6412\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.9430 - loss: 0.1384 - val_accuracy: 0.8152 - val_loss: 0.6583\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 45ms/step - accuracy: 0.9436 - loss: 0.1412 - val_accuracy: 0.8177 - val_loss: 0.6596\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.9461 - loss: 0.1359 - val_accuracy: 0.8166 - val_loss: 0.6684\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
            "TF-IDF Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84      2996\n",
            "           1       0.81      0.76      0.79      2346\n",
            "\n",
            "    accuracy                           0.82      5342\n",
            "   macro avg       0.82      0.81      0.81      5342\n",
            "weighted avg       0.82      0.82      0.82      5342\n",
            "\n",
            "\n",
            "Running GloVe Method with GRU:\n",
            "\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 250ms/step - accuracy: 0.5654 - loss: 0.6873 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 252ms/step - accuracy: 0.5654 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 241ms/step - accuracy: 0.5605 - loss: 0.6861 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 242ms/step - accuracy: 0.5593 - loss: 0.6863 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 243ms/step - accuracy: 0.5569 - loss: 0.6868 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 247ms/step - accuracy: 0.5620 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 257ms/step - accuracy: 0.5628 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 244ms/step - accuracy: 0.5595 - loss: 0.6863 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 244ms/step - accuracy: 0.5652 - loss: 0.6849 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 247ms/step - accuracy: 0.5674 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 245ms/step - accuracy: 0.5622 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 245ms/step - accuracy: 0.5658 - loss: 0.6848 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 246ms/step - accuracy: 0.5583 - loss: 0.6864 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 248ms/step - accuracy: 0.5623 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 258ms/step - accuracy: 0.5679 - loss: 0.6841 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 251ms/step - accuracy: 0.5556 - loss: 0.6871 - val_accuracy: 0.5538 - val_loss: 0.6888\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 255ms/step - accuracy: 0.5614 - loss: 0.6862 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 254ms/step - accuracy: 0.5633 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 247ms/step - accuracy: 0.5643 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 253ms/step - accuracy: 0.5527 - loss: 0.6876 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 256ms/step - accuracy: 0.5622 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 254ms/step - accuracy: 0.5675 - loss: 0.6841 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 252ms/step - accuracy: 0.5637 - loss: 0.6851 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 249ms/step - accuracy: 0.5648 - loss: 0.6849 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 246ms/step - accuracy: 0.5609 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 251ms/step - accuracy: 0.5611 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 246ms/step - accuracy: 0.5636 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 252ms/step - accuracy: 0.5657 - loss: 0.6846 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 254ms/step - accuracy: 0.5618 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 251ms/step - accuracy: 0.5674 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 248ms/step - accuracy: 0.5629 - loss: 0.6854 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 245ms/step - accuracy: 0.5642 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 247ms/step - accuracy: 0.5610 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 354ms/step - accuracy: 0.5698 - loss: 0.6836 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 250ms/step - accuracy: 0.5676 - loss: 820.1008 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 250ms/step - accuracy: 0.5653 - loss: 0.6846 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 256ms/step - accuracy: 0.5641 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 250ms/step - accuracy: 0.5649 - loss: 0.6848 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 254ms/step - accuracy: 0.5610 - loss: 0.6858 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 255ms/step - accuracy: 0.5599 - loss: 0.6860 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 247ms/step - accuracy: 0.5553 - loss: 0.6872 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 252ms/step - accuracy: 0.5641 - loss: 0.6849 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 254ms/step - accuracy: 0.5635 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 255ms/step - accuracy: 0.5640 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 253ms/step - accuracy: 0.5650 - loss: 8861.5518 - val_accuracy: 0.5538 - val_loss: 0.6891\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 247ms/step - accuracy: 0.5615 - loss: 0.6865 - val_accuracy: 0.5538 - val_loss: 0.6882\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 241ms/step - accuracy: 0.5661 - loss: 0.6844 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 246ms/step - accuracy: 0.5575 - loss: 441.7217 - val_accuracy: 0.5538 - val_loss: 0.6881\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 247ms/step - accuracy: 0.5516 - loss: 0.6886 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 250ms/step - accuracy: 0.5640 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step\n",
            "GloVe Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72      2996\n",
            "           1       0.00      0.00      0.00      2346\n",
            "\n",
            "    accuracy                           0.56      5342\n",
            "   macro avg       0.28      0.50      0.36      5342\n",
            "weighted avg       0.31      0.56      0.40      5342\n",
            "\n",
            "\n",
            "Running Word2Vec Method with GRU:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 293ms/step - accuracy: 0.5637 - loss: 0.6869 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 2/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 304ms/step - accuracy: 0.5628 - loss: 0.6857 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 3/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 297ms/step - accuracy: 0.5591 - loss: 0.6864 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 4/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 294ms/step - accuracy: 0.5642 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 5/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 297ms/step - accuracy: 0.5580 - loss: 0.6868 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 6/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 302ms/step - accuracy: 0.5642 - loss: 0.6854 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 7/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 294ms/step - accuracy: 0.5681 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 8/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 291ms/step - accuracy: 0.5629 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 9/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 301ms/step - accuracy: 0.5680 - loss: 0.6842 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 10/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 317ms/step - accuracy: 0.5671 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 11/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 295ms/step - accuracy: 0.5580 - loss: 0.6866 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 12/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 295ms/step - accuracy: 0.5575 - loss: 0.6869 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 13/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 304ms/step - accuracy: 0.5615 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 14/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 297ms/step - accuracy: 0.5547 - loss: 0.6873 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 15/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 294ms/step - accuracy: 0.5633 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6879\n",
            "Epoch 16/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 303ms/step - accuracy: 0.5624 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 17/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 297ms/step - accuracy: 0.5689 - loss: 0.6836 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 18/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 299ms/step - accuracy: 0.5684 - loss: 0.6840 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 19/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 303ms/step - accuracy: 0.5639 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6896\n",
            "Epoch 20/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 295ms/step - accuracy: 0.5654 - loss: 0.6848 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 21/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 297ms/step - accuracy: 0.5702 - loss: 0.6834 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 22/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 321ms/step - accuracy: 0.5649 - loss: 0.6851 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 23/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 293ms/step - accuracy: 0.5667 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 24/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 304ms/step - accuracy: 0.5628 - loss: 0.6854 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 25/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 296ms/step - accuracy: 0.5681 - loss: 0.6839 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 26/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 295ms/step - accuracy: 0.5672 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 27/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 296ms/step - accuracy: 0.5610 - loss: 20543.5645 - val_accuracy: 0.5538 - val_loss: 0.6883\n",
            "Epoch 28/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 320ms/step - accuracy: 0.5618 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 29/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 294ms/step - accuracy: 0.5630 - loss: 0.6855 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 30/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 303ms/step - accuracy: 0.5612 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 31/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 303ms/step - accuracy: 0.5651 - loss: 0.6850 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 32/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 294ms/step - accuracy: 0.5618 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 33/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 306ms/step - accuracy: 0.5547 - loss: 0.6875 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 34/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 290ms/step - accuracy: 0.5620 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6880\n",
            "Epoch 35/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 294ms/step - accuracy: 0.5635 - loss: 0.6853 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 36/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 293ms/step - accuracy: 0.5687 - loss: 0.6839 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 37/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 319ms/step - accuracy: 0.5654 - loss: 0.6847 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 38/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 295ms/step - accuracy: 0.5710 - loss: 0.6834 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 39/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 299ms/step - accuracy: 0.5677 - loss: 0.6845 - val_accuracy: 0.5538 - val_loss: 0.6873\n",
            "Epoch 40/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 303ms/step - accuracy: 0.5621 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 41/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 297ms/step - accuracy: 0.5636 - loss: 0.6852 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 42/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 305ms/step - accuracy: 0.5676 - loss: 0.6840 - val_accuracy: 0.5538 - val_loss: 0.6878\n",
            "Epoch 43/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 301ms/step - accuracy: 0.5577 - loss: 0.6867 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 44/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 300ms/step - accuracy: 0.5598 - loss: 0.6861 - val_accuracy: 0.5538 - val_loss: 0.6877\n",
            "Epoch 45/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 312ms/step - accuracy: 0.5633 - loss: 0.6851 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 46/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 297ms/step - accuracy: 0.5667 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6874\n",
            "Epoch 47/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 305ms/step - accuracy: 0.5667 - loss: 0.6843 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 48/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 305ms/step - accuracy: 0.5605 - loss: 0.6859 - val_accuracy: 0.5538 - val_loss: 0.6876\n",
            "Epoch 49/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 296ms/step - accuracy: 0.5621 - loss: 0.6856 - val_accuracy: 0.5538 - val_loss: 0.6875\n",
            "Epoch 50/50\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 306ms/step - accuracy: 0.5627 - loss: 0.6853 - val_accuracy: 0.5538 - val_loss: 0.6883\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step\n",
            "Word2Vec Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      1.00      0.72      2996\n",
            "           1       0.00      0.00      0.00      2346\n",
            "\n",
            "    accuracy                           0.56      5342\n",
            "   macro avg       0.28      0.50      0.36      5342\n",
            "weighted avg       0.31      0.56      0.40      5342\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')       # Corrected from 'punkt_tab' to 'punkt'\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset from Excel file\n",
        "data = pd.read_excel(\"Sarcasm_Headlines_Dataset.xlsx\")\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "data['tokens'] = data['headline'].apply(preprocess_text)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X = data['headline']\n",
        "y = data['is_sarcastic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize train and test sets\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sequences\n",
        "max_sequence_length = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 1. TF-IDF with GRU\n",
        "def tfidf_gru_model():\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    # Reshape TF-IDF output to 3D for GRU (samples, timesteps, features)\n",
        "    X_train_tfidf = np.expand_dims(X_train_tfidf, axis=1)\n",
        "    X_test_tfidf = np.expand_dims(X_test_tfidf, axis=1)\n",
        "\n",
        "    model = Sequential([\n",
        "        GRU(128, input_shape=(X_train_tfidf.shape[1], X_train_tfidf.shape[2]), dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_tfidf, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "    y_pred = (model.predict(X_test_tfidf) > 0.5).astype(\"int32\")\n",
        "    print(\"TF-IDF Classification Report:\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 2. GloVe Embeddings with GRU\n",
        "def create_glove_embedding_matrix():\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-100\")  # Ensure you have internet connection for downloading\n",
        "    embedding_dim = 100\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in glove_model:\n",
        "            embedding_matrix[i] = glove_model[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "def glove_gru_model():\n",
        "    embedding_matrix = create_glove_embedding_matrix()\n",
        "    embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "                  input_length=max_sequence_length, trainable=False),\n",
        "        SpatialDropout1D(0.2),\n",
        "        GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. Word2Vec Embeddings with GRU\n",
        "def create_word2vec_embedding_matrix():\n",
        "    w2v_model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "    embedding_dim = w2v_model.vector_size\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = w2v_model.wv[word]\n",
        "    return embedding_matrix\n",
        "\n",
        "def word2vec_gru_model():\n",
        "    embedding_matrix = create_word2vec_embedding_matrix()\n",
        "    embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix],\n",
        "                  input_length=max_sequence_length, trainable=True),\n",
        "        SpatialDropout1D(0.2),\n",
        "        GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and Evaluate Models\n",
        "def train_and_evaluate_gru(method):\n",
        "    if method == \"TF-IDF\":\n",
        "        tfidf_gru_model()\n",
        "    elif method == \"GloVe\":\n",
        "        model = glove_gru_model()\n",
        "        model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "        y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "        print(\"GloVe Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "    elif method == \"Word2Vec\":\n",
        "        model = word2vec_gru_model()\n",
        "        model.fit(X_train_pad, y_train, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "        y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
        "        print(\"Word2Vec Classification Report:\\n\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "    else:\n",
        "        print(\"Invalid method. Choose from 'TF-IDF', 'GloVe', or 'Word2Vec'.\")\n",
        "\n",
        "# Run all methods\n",
        "methods = [\"TF-IDF\", \"GloVe\", \"Word2Vec\"]\n",
        "for method in methods:\n",
        "    print(f\"\\nRunning {method} Method with GRU:\\n\")\n",
        "    train_and_evaluate_gru(method)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer**"
      ],
      "metadata": {
        "id": "hrSbCXeJlWxE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D9JMgN6SwiL",
        "outputId": "2d69e06a-d23b-4ea0-c8af-5461e1f25171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pandas transformers datasets torch scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEaHYrlLSc0r",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load data from Excel\n",
        "file_path = \"/content/Sarcasm_Headlines_Dataset.xlsx\"  # Replace with your file path\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Ensure proper column names\n",
        "df.columns = df.columns.str.strip()\n",
        "assert 'headline' in df.columns and 'is_sarcastic' in df.columns, \"Columns 'headline' and 'is_sarcastic' are required\"\n",
        "\n",
        "# Split data into train and test\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['headline'].tolist(),\n",
        "    df['is_sarcastic'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize data\n",
        "def preprocess_data(texts, labels):\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
        "    return {'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask'],\n",
        "            'labels': labels}\n",
        "\n",
        "train_encodings = preprocess_data(train_texts, train_labels)\n",
        "val_encodings = preprocess_data(val_texts, val_labels)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict(train_encodings)\n",
        "val_dataset = Dataset.from_dict(val_encodings)\n",
        "\n",
        "# Load pre-trained model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Output directory\n",
        "    evaluation_strategy=\"epoch\",    # Evaluate after each epoch\n",
        "    per_device_train_batch_size=16, # Batch size for training\n",
        "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
        "    num_train_epochs=3,             # Total number of training epochs\n",
        "    learning_rate=5e-5,             # Learning rate\n",
        "    weight_decay=0.01,              # Weight decay\n",
        "    logging_dir=\"./logs\",           # Log directory\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # Add compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./sarcasm_model\")\n",
        "tokenizer.save_pretrained(\"./sarcasm_model\")\n",
        "\n",
        "print(\"Model training completed and saved!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}